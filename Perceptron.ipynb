{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b49ac3d-d542-44d6-9613-04df0c6258da",
   "metadata": {},
   "source": [
    "\n",
    "![\"process\"](etl_process_styled.gif)\n",
    "Le script charge le dataset, nettoie/encode diagnosis en label (1/-1), assemble les features en listes float. \n",
    "Puis entraîne un perceptron distribué implémenté en Estimator/Model Spark ML :\n",
    "à chaque époque il prédit (UDF), sélectionne les erreurs, agrège via RDD map/reduce les mises à jour et applique : \n",
    "``` bash\n",
    "w += lr * sum(y*x) / b += lr * sum(y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c607de-4dd9-4ade-a79e-c93474d8ce3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes non numériques détectées: ['_c32']\n",
      "Epoch 1/20 -> erreurs=-357\n",
      "Epoch 2/20 -> erreurs=-357\n",
      "Epoch 3/20 -> erreurs=-357\n",
      "Epoch 4/20 -> erreurs=-357\n",
      "Epoch 5/20 -> erreurs=-357\n",
      "Epoch 6/20 -> erreurs=-357\n",
      "Epoch 7/20 -> erreurs=-357\n",
      "Epoch 8/20 -> erreurs=-357\n",
      "Epoch 9/20 -> erreurs=-357\n",
      "Epoch 10/20 -> erreurs=-357\n",
      "Epoch 11/20 -> erreurs=-357\n",
      "Epoch 12/20 -> erreurs=-357\n",
      "Epoch 13/20 -> erreurs=-357\n",
      "Epoch 14/20 -> erreurs=-357\n",
      "Epoch 15/20 -> erreurs=-357\n",
      "Epoch 16/20 -> erreurs=-357\n",
      "Epoch 17/20 -> erreurs=-357\n",
      "Epoch 18/20 -> erreurs=-357\n",
      "Epoch 19/20 -> erreurs=-357\n",
      "Epoch 20/20 -> erreurs=-357\n",
      "+-----+--------------------+----------+\n",
      "|label|            features|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|    1|[17.99, 10.38, 12...|         1|\n",
      "|    1|[20.57, 17.77, 13...|         1|\n",
      "|    1|[19.69, 21.25, 13...|         1|\n",
      "|    1|[11.42, 20.38, 77...|         1|\n",
      "|    1|[20.29, 14.34, 13...|         1|\n",
      "|    1|[12.45, 15.7, 82....|         1|\n",
      "|    1|[18.25, 19.98, 11...|         1|\n",
      "|    1|[13.71, 20.83, 90...|         1|\n",
      "|    1|[13.0, 21.82, 87....|         1|\n",
      "|    1|[12.46, 24.04, 83...|         1|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "✅ Accuracy = 37.26%\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# 1. Initialisation Spark\n",
    "# ========================================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BCWPerceptron\").getOrCreate()\n",
    "\n",
    "# ========================================================================\n",
    "# 2. Chargement et nettoyage des données\n",
    "# ========================================================================\n",
    "# Charger le fichier CSV avec les en-têtes et inférence des types\n",
    "df = spark.read.csv(\"bcw_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Supprimer colonnes inutiles : \"id\" et la colonne vide \"Unnamed: 32\"\n",
    "df = df.drop(\"id\", \"_c32\")\n",
    "\n",
    "# Créer la colonne label : \"M\" = 1, \"B\" = -1\n",
    "df = df.withColumn(\"label\", when(col(\"diagnosis\") == \"M\", 1).otherwise(-1))\n",
    "\n",
    "# Supprimer la colonne diagnosis (inutile après transformation)\n",
    "df = df.drop(\"diagnosis\")\n",
    "\n",
    "# Vérifier si certaines colonnes ne sont pas numériques\n",
    "bad_cols = [c for c, t in df.dtypes if t not in (\"double\", \"int\")]\n",
    "print(\"Colonnes non numériques détectées:\", bad_cols)\n",
    "\n",
    "# Supprimer toute colonne non numérique restante\n",
    "df = df.drop(*bad_cols)\n",
    "\n",
    "# ========================================================================\n",
    "# 3. Assemblage des features\n",
    "# ========================================================================\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# Sélectionner toutes les colonnes sauf \"label\" comme features\n",
    "feature_cols = [c for c in df.columns if c != \"label\"]\n",
    "\n",
    "# Transformer en vecteur Spark ML\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "\n",
    "# UDF pour convertir DenseVector Spark -> liste Python (numpy friendly)\n",
    "def to_array(v):\n",
    "    return [float(x) for x in v]\n",
    "\n",
    "to_array_udf = udf(to_array, ArrayType(DoubleType()))\n",
    "\n",
    "# Ajouter une colonne \"features\" au format tableau (list de floats)\n",
    "df = assembler.transform(df) \\\n",
    "              .withColumn(\"features\", to_array_udf(col(\"features_vec\"))) \\\n",
    "              .select(\"label\", \"features\")\n",
    "\n",
    "# ========================================================================\n",
    "# 4. Implémentation du Perceptron\n",
    "# ========================================================================\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Estimator, Model, Pipeline\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.param.shared import Param, Params, TypeConverters\n",
    "\n",
    "# ---- Modèle final (Transformer) ----\n",
    "class PerceptronModel(Model, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, weights, bias):\n",
    "        super(PerceptronModel, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def _predict_udf(self):\n",
    "        w = self.weights\n",
    "        b = self.bias\n",
    "        def predict(features):\n",
    "            return 1 if np.dot(w, features) + b >= 0 else -1\n",
    "        return udf(predict, IntegerType())\n",
    "\n",
    "    def _transform(self, df):\n",
    "        predict_udf = self._predict_udf()\n",
    "        return df.withColumn(\"prediction\", predict_udf(col(\"features\")))\n",
    "\n",
    "# ---- Estimateur (entraîneur) ----\n",
    "class PerceptronClassifier(Estimator, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    lr = Param(Params._dummy(), \"lr\", \"learning rate\", typeConverter=TypeConverters.toFloat)\n",
    "    epochs = Param(Params._dummy(), \"epochs\", \"number of epochs\", typeConverter=TypeConverters.toInt)\n",
    "\n",
    "    def __init__(self, lr=0.1, epochs=10):\n",
    "        super(PerceptronClassifier, self).__init__()\n",
    "        self._setDefault(lr=0.1, epochs=10)\n",
    "        self._set(lr=lr, epochs=epochs)\n",
    "\n",
    "    def _fit(self, df):\n",
    "        lr = self.getOrDefault(self.lr)\n",
    "        epochs = self.getOrDefault(self.epochs)\n",
    "\n",
    "        # Taille du vecteur de features\n",
    "        dim = len(df.first()[\"features\"])\n",
    "        w = np.zeros(dim)\n",
    "        b = 0.0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Calcul des prédictions locales\n",
    "            def predict_local(features):\n",
    "                return 1 if np.dot(w, features) + b >= 0 else -1\n",
    "            predict_udf = udf(predict_local, IntegerType())\n",
    "            df_pred = df.withColumn(\"prediction\", predict_udf(col(\"features\")))\n",
    "\n",
    "            # Identifier les erreurs de classification\n",
    "            df_err = df_pred.withColumn(\n",
    "                \"error\", when(col(\"label\") * col(\"prediction\") <= 0, lit(1)).otherwise(lit(0))\n",
    "            )\n",
    "\n",
    "            # Calcul des mises à jour via RDD\n",
    "            updates = df_err.rdd.map(lambda row: (\n",
    "                row[\"error\"] * row[\"label\"],  \n",
    "                np.array(row[\"features\"]) if row[\"error\"] == 1 else np.zeros(dim)\n",
    "            ))\n",
    "\n",
    "            # Somme des corrections\n",
    "            total_update = updates.reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "            # Mise à jour des poids et du biais\n",
    "            b += lr * total_update[0]\n",
    "            w += lr * total_update[1]\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} -> erreurs={total_update[0]}\")\n",
    "\n",
    "        return PerceptronModel(weights=w, bias=b)\n",
    "\n",
    "# ========================================================================\n",
    "# 5. Pipeline et entraînement\n",
    "# ========================================================================\n",
    "perceptron = PerceptronClassifier(lr=0.01, epochs=20)\n",
    "pipeline = Pipeline(stages=[perceptron])\n",
    "\n",
    "# Entraîner le modèle\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# Prédictions sur le dataset\n",
    "df_pred = model.transform(df)\n",
    "df_pred.show(10)\n",
    "\n",
    "# ========================================================================\n",
    "# 6. Évaluation : Accuracy\n",
    "# ========================================================================\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "accuracy = df_pred.withColumn(\n",
    "    \"correct\", when(col(\"label\") == col(\"prediction\"), 1).otherwise(0)\n",
    ").agg(avg(\"correct\")).collect()[0][0]\n",
    "\n",
    "print(f\"✅ Accuracy = {accuracy*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
