{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d496442c-bcc1-48a7-b7a6-7658fe731744",
   "metadata": {},
   "source": [
    "# Conversion d‚Äôun fichier CSV en Parquet avec PySpark\r\n",
    "\r\n",
    "## üéØ Objectif\r\n",
    "Ce script a pour but de **convertir un fichier CSV en format Parquet** en utilisant **PySpark**.  \r\n",
    "Le format **Parquet** est un format **colonnaire**, optimis√© pour le traitement distribu√© et l‚Äôanalyse de donn√©es massives.\r\n",
    "\r\n",
    "## üöÄ Pourquoi passer de CSV √† Parquet ?\r\n",
    "- **Performance** : Parquet est beaucoup plus rapide √† lire/√©crire que CSV gr√¢ce √† son organisation par colonnes.\r\n",
    "- **Compression** : Le format Parquet applique automatiquement une compression efficace, r√©duisant la taille des donn√©es stock√©es.\r\n",
    "- **Sch√©ma explicite** : Contrairement au CSV qui est purement textuel, Parquet conserve le sch√©ma (types de colonnes).\r\n",
    "- **Compatibilit√©** : C‚Äôest le format standard utilis√© par Spark, Hadoop, Hive, et la plupart des outils Big Data.\r\n",
    "\r\n",
    "## üõ†Ô∏è Fonctionnement du script\r\n",
    "1. Cr√©ation d‚Äôune **session Spark**.\r\n",
    "2. Lecture du fichier CSV avec d√©tection automatique du sch√©ma (`inferSchema=True`).\r\n",
    "3. √âcriture du DataFrame en **format Parquet**, avec ou sans partitionnement selon la pr√©sence de la colonne `id`.\r\n",
    "4. Sauvegarde des donn√©es dans un r√©pertoire de sortie.\r\n",
    "5. Arr√™t de la session Spark.\r\n",
    "\r\n",
    "## üìÇ R√©sultat\r\n",
    "Le script g√©n√®re un **dossier Parquet**, contenant :\r\n",
    "- Des fichiers `.parquet` (un par partition ou par t√¢che Spark).\r\n",
    "- Un fichier `_SUCCESS` qui indique que le job Spark s‚Äôest termin√© correctement.\r\n",
    "\r\n",
    "Exemple de structure :\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02742681-83e5-4ac3-910b-6df5725b00f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- diagnosis: string (nullable = true)\n",
      " |-- radius_mean: double (nullable = true)\n",
      " |-- texture_mean: double (nullable = true)\n",
      " |-- perimeter_mean: double (nullable = true)\n",
      " |-- area_mean: double (nullable = true)\n",
      " |-- smoothness_mean: double (nullable = true)\n",
      " |-- compactness_mean: double (nullable = true)\n",
      " |-- concavity_mean: double (nullable = true)\n",
      " |-- concave points_mean: double (nullable = true)\n",
      " |-- symmetry_mean: double (nullable = true)\n",
      " |-- fractal_dimension_mean: double (nullable = true)\n",
      " |-- radius_se: double (nullable = true)\n",
      " |-- texture_se: double (nullable = true)\n",
      " |-- perimeter_se: double (nullable = true)\n",
      " |-- area_se: double (nullable = true)\n",
      " |-- smoothness_se: double (nullable = true)\n",
      " |-- compactness_se: double (nullable = true)\n",
      " |-- concavity_se: double (nullable = true)\n",
      " |-- concave points_se: double (nullable = true)\n",
      " |-- symmetry_se: double (nullable = true)\n",
      " |-- fractal_dimension_se: double (nullable = true)\n",
      " |-- radius_worst: double (nullable = true)\n",
      " |-- texture_worst: double (nullable = true)\n",
      " |-- perimeter_worst: double (nullable = true)\n",
      " |-- area_worst: double (nullable = true)\n",
      " |-- smoothness_worst: double (nullable = true)\n",
      " |-- compactness_worst: double (nullable = true)\n",
      " |-- concavity_worst: double (nullable = true)\n",
      " |-- concave points_worst: double (nullable = true)\n",
      " |-- symmetry_worst: double (nullable = true)\n",
      " |-- fractal_dimension_worst: double (nullable = true)\n",
      " |-- _c32: string (nullable = true)\n",
      "\n",
      "Conversion termin√©e. Les fichiers Parquet sont enregistr√©s dans : /home/jovyan/parquet_output\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Cr√©er une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV to Parquet Conversion\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Chemin du fichier CSV d'entr√©e\n",
    "input_csv_path = \"bcw_data.csv\"\n",
    "\n",
    "# Chemin du r√©pertoire de sortie pour les fichiers Parquet Utiliser un chemin absolu mais dans mon $HOME (droits d'√©criture)\n",
    "output_parquet_path = os.path.expanduser(\"~/parquet_output\")\n",
    "\n",
    "# Lire le fichier CSV en DataFrame\n",
    "# Vous pouvez ajuster les options selon votre fichier (ex: s√©parateur, en-t√™tes, etc.)\n",
    "df = spark.read.csv(input_csv_path, header=True, inferSchema=True)\n",
    "\n",
    "# V√©rifier le sch√©ma du DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Partitionner les donn√©es par une ou plusieurs colonnes (exemple : \"colonne_partition\")\n",
    "partition_column = \"id\"  # Remplacez par le nom de votre colonne\n",
    "if partition_column in df.columns:\n",
    "    # √âcrire le DataFrame en format Parquet avec partitionnement\n",
    "    df.write \\\n",
    "        .partitionBy(partition_column) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(output_parquet_path)\n",
    "else:\n",
    "    # Si la colonne de partition n'existe pas, √©crire sans partitionnement\n",
    "    df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(output_parquet_path)\n",
    "\n",
    "print(f\"Conversion termin√©e. Les fichiers Parquet sont enregistr√©s dans : {output_parquet_path}\")\n",
    "\n",
    "# Arr√™ter la session Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "592b5ccb-735a-4526-af72-c240b62ab56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4273d624-b47d-48bd-8e43-fb56fc7a7377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, diagnosis: string, radius_mean: double, texture_mean: double, perimeter_mean: double, area_mean: double, smoothness_mean: double, compactness_mean: double, concavity_mean: double, concave points_mean: double, symmetry_mean: double, fractal_dimension_mean: double, radius_se: double, texture_se: double, perimeter_se: double, area_se: double, smoothness_se: double, compactness_se: double, concavity_se: double, concave points_se: double, symmetry_se: double, fractal_dimension_se: double, radius_worst: double, texture_worst: double, perimeter_worst: double, area_worst: double, smoothness_worst: double, compactness_worst: double, concavity_worst: double, concave points_worst: double, symmetry_worst: double, fractal_dimension_worst: double, _c32: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf84d00-6507-4a10-b868-df4784a4d23d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
